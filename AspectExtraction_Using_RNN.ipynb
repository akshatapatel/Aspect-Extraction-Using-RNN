{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Extraction Using RNN and Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word and its Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129218\n"
     ]
    }
   ],
   "source": [
    "word_vectors=[]\n",
    "words=[]\n",
    "unique_words=[]\n",
    "vectors = []\n",
    "f = open('word2vec.csv','r')\n",
    "word_vectors = f.read()\n",
    "\n",
    "word_vectors = word_vectors.split('\\n',1)[1]\n",
    "word_vectors = word_vectors.replace(',\"',':')\n",
    "word_vectors = word_vectors.replace('\\n','')\n",
    "word_vectors = word_vectors.split('\"')\n",
    "word_vectors = word_vectors[:-1]\n",
    "print(len(word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But:[-0.22266774  0.04329333  0.37159213 -0.5329994   0.6239861  -0.24587387  0.34643427  0.13116741 -0.41579244 -0.20065007  0.08610805  0.6125774  0.20295028 -0.12502426 -0.30027688  0.16528115 -0.6238571  -0.03565881 -0.17556378  0.11841667  0.5494477  -0.6912074   0.11358704  0.01227758 -0.3365192  -0.09856282  0.20129     0.29659027 -0.1969936   0.15831183 -0.057485   -0.0241813 ]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into individual words and vectors list\n",
    "\n",
    "for word_vec in word_vectors:\n",
    "    vectors.append(word_vec.split(':')[1])\n",
    "    words.append(word_vec.split(':')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22266774  0.04329333  0.37159213 -0.5329994   0.6239861  -0.24587387  0.34643427  0.13116741 -0.41579244 -0.20065007  0.08610805  0.6125774  0.20295028 -0.12502426 -0.30027688  0.16528115 -0.6238571  -0.03565881 -0.17556378  0.11841667  0.5494477  -0.6912074   0.11358704  0.01227758 -0.3365192  -0.09856282  0.20129     0.29659027 -0.1969936   0.15831183 -0.057485   -0.0241813 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of unique words\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "dictionary_words = dict()\n",
    "for entry in words:\n",
    "    if entry.lower() not in unique_words:\n",
    "        unique_words.append(entry.lower())\n",
    "for word in unique_words:\n",
    "    dictionary_words[word.lower()] = len(dictionary_words)\n",
    "\n",
    "#print (len(dictionary_words))  #5257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3121,But the staff was so horrible to us.,staff,negative,8,13,,,,,,,staff,,,,staff \n"
     ]
    }
   ],
   "source": [
    "f = open('data.csv','r')\n",
    "temp = f.read()\n",
    "temp=temp.split('\\n')\n",
    "temp = temp[1:]\n",
    "print(temp[0])\n",
    "\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp in temp:\n",
    "    tmp = tmp.split(',')\n",
    "    labels.append(tmp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7408"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels[:-1]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7408\n",
      "1215\n"
     ]
    }
   ],
   "source": [
    "#creating a list of unique labels\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv('data.csv');\n",
    "term=df['combine']  # terms are the labels\n",
    "unique_terms=[]   # new_terms = unique_terms\n",
    "#term\n",
    "term = [word.replace(' ','') for word in term]\n",
    "for entry in term:\n",
    "    if entry.lower() not in unique_terms:\n",
    "        unique_terms.append(entry.lower())\n",
    "        \n",
    "print(len(term))   #7408\n",
    "print(len(unique_terms))     #1215 unique labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(unique_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1215\n"
     ]
    }
   ],
   "source": [
    "#creating a dictionary of numbers for each label in the corpus\n",
    "\n",
    "#count = collections.Counter(unique_terms).most_common()\n",
    "dictionary = dict()\n",
    "for word in unique_terms:\n",
    "    dictionary[word.lower()] = len(dictionary)\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "print (len(reverse_dictionary))  #1215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a integer list of the words from the dictionary\n",
    "\n",
    "df=pd.read_csv('data.csv');\n",
    "text=df['text'] \n",
    "text_numbers = []\n",
    "for txt in text:\n",
    "    #print (text)\n",
    "    txt=txt.replace('.','')\n",
    "    txt=txt.replace('comma','')\n",
    "    txt=txt.replace('\\'','')\n",
    "    all_words=txt.split(' ')\n",
    "    #print(all_words)\n",
    "    for i in range(len(all_words)):\n",
    "        if all_words[i].lower() in dictionary_words:\n",
    "            all_words[i]=dictionary_words[all_words[i].lower()]\n",
    "        else:\n",
    "            all_words[i]=0\n",
    "    #print(all_words)\n",
    "    if len(all_words)<32:\n",
    "        for y in range(32-len(all_words)):\n",
    "            all_words.append(0)\n",
    "    elif len(all_words)>32:\n",
    "        all_words=all_words[0:32]\n",
    "    text_numbers.append(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7408"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_numbers = np.array(text_numbers).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7408"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for trm in term:\n",
    "    if trm.lower() in dictionary:\n",
    "        y_train.append(dictionary[trm.lower()])\n",
    "    else:\n",
    "        y_train = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7408"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unique = []\n",
    "for trm in unique_terms:\n",
    "    if trm.lower() in dictionary:\n",
    "        y_train_unique.append(dictionary[trm.lower()])\n",
    "    else:\n",
    "        y_train_unique = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the paramters for the network\n",
    "\n",
    "num_epochs = 10\n",
    "total_series_length = 7400\n",
    "truncated_backprop_length = 32\n",
    "state_size = 1\n",
    "num_classes = 1215\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = int(total_series_length//batch_size//truncated_backprop_length)\n",
    "\n",
    "#defining the placeholders for the model\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, 1])\n",
    "\n",
    "\n",
    "# the weight and the bias\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x,y):\n",
    "    #tf.reset_default_graph()\n",
    "    \n",
    "    # Unpack columns\n",
    "    \n",
    "    inputs_series = tf.split(batchX_placeholder, truncated_backprop_length, axis=1)\n",
    "    labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "    # Forward passes\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "    states_series, current_state = tf.nn.static_rnn(cell = cell, inputs = inputs_series,initial_state = init_state)\n",
    "        \n",
    "    logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "    predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "    \n",
    "\n",
    "    losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "    train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "   \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "            print(\"\\nNew data, epoch\", epoch_idx)\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * truncated_backprop_length\n",
    "                end_idx = start_idx + truncated_backprop_length\n",
    "                \n",
    "                batchX = x[:,start_idx:end_idx]\n",
    "                batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "                _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                    [total_loss, train_step, current_state, predictions_series],\n",
    "                    feed_dict={\n",
    "                        batchX_placeholder:batchX,\n",
    "                        batchY_placeholder:batchY,\n",
    "                        init_state:_current_state\n",
    "                    })\n",
    "\n",
    "                loss_list.append(_total_loss)\n",
    "\n",
    "                if batch_idx%20 == 0:\n",
    "                    print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-3e31eaf81bda>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /anaconda2/envs/Python36/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "\n",
      "New data, epoch 0\n",
      "Step 0 Loss 7.1242743\n",
      "Step 20 Loss 5.190398\n",
      "Step 40 Loss 4.7921286\n",
      "\n",
      "New data, epoch 1\n",
      "Step 0 Loss 4.620501\n",
      "Step 20 Loss 3.9734256\n",
      "Step 40 Loss 4.4526715\n",
      "\n",
      "New data, epoch 2\n",
      "Step 0 Loss 4.398009\n",
      "Step 20 Loss 3.8414574\n",
      "Step 40 Loss 4.3799043\n",
      "\n",
      "New data, epoch 3\n",
      "Step 0 Loss 4.3495817\n",
      "Step 20 Loss 3.7875156\n",
      "Step 40 Loss 4.330928\n",
      "\n",
      "New data, epoch 4\n",
      "Step 0 Loss 4.3152785\n",
      "Step 20 Loss 3.7496612\n",
      "Step 40 Loss 4.2923374\n",
      "\n",
      "New data, epoch 5\n",
      "Step 0 Loss 4.2882667\n",
      "Step 20 Loss 3.72107\n",
      "Step 40 Loss 4.2620177\n",
      "\n",
      "New data, epoch 6\n",
      "Step 0 Loss 4.2671094\n",
      "Step 20 Loss 3.6993968\n",
      "Step 40 Loss 4.238047\n",
      "\n",
      "New data, epoch 7\n",
      "Step 0 Loss 4.250466\n",
      "Step 20 Loss 3.6828933\n",
      "Step 40 Loss 4.21874\n",
      "\n",
      "New data, epoch 8\n",
      "Step 0 Loss 4.237222\n",
      "Step 20 Loss 3.670138\n",
      "Step 40 Loss 4.2028575\n",
      "\n",
      "New data, epoch 9\n",
      "Step 0 Loss 4.226541\n",
      "Step 20 Loss 3.6600366\n",
      "Step 40 Loss 4.189561\n",
      "\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "text_numbers=text_numbers[0:7400]\n",
    "y_train = y_train[0:7400]\n",
    "tn_copy = text_numbers\n",
    "tn_copy = tn_copy.reshape([batch_size,-1])\n",
    "lb_copy = []\n",
    "count = int(len(y_train)/batch_size)  #1480\n",
    "cnt = 0\n",
    "j = 0\n",
    "\n",
    "while j < batch_size:\n",
    "    lb = []\n",
    "    for counter in range(count):\n",
    "        lb.append(y_train[cnt+counter])\n",
    "        \n",
    "    lb_copy.append(lb)\n",
    "    cnt+=count\n",
    "    j+=1\n",
    "lb_copy = np.array(lb_copy)\n",
    "\n",
    "train_neural_network(tn_copy,lb_copy)\n",
    "\n",
    "print(\"\\nTraining done\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
